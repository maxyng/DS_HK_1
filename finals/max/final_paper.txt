
Overview of problem and hypothesis
Overview of data
Modeling techniques used and why
What decisions your findings allow you to make.



Description of problem and hypothesis.
--------------------------------

The problem to tackle for my final project is from an ongoing kaggle competition.  The official description is below.

"The Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging."

In my words, the problem is to be able to judge if the review written is giving a positive or negative opinion on the film based on any key words that would determine if the reviewer thinks highly of the film or not.

What I feel is, there will be words that greatly determine if the opinion given on the film is positive or negative and sarcastic adjectives will not sway the analysis greatly.

Detailed description your data set.
--------------------------------

The data provided by Kaggle does not have many features.  There are only four features:

PhraseId 
SentenceId
Phrase
Sentiment

The unique identifier is "PhraseId" to distinguish the different rows.  This way it's being distinguished is by a numerical sequential number that increments by 1.  This is a standard for identifying each sentence uniquely.  

The SentenceId is what makes this dataset special.  It is what groups the different rows together so the data scientist knows they belong to each other.  There is a main sentence that is broken up into phrases which appears to be randomly broken up.  One very unique thing about the phrases is that when the "'s" is written, there is a space between the word and "'s" causing string tokenizers to misinterpret "'s" as a single word.  There were over thirteen thousand instances of "'s" singilarily appearing.  Commas and other punctuation symbols also appear in the dataset.

The sentiment is the score the reviewer put along with the descriptive sentence describing their feeling of the film.  The mapping is as follows

negative: 0
somewhat negative: 1
neutral: 2
somewhat positive: 3
positive: 4

There are 156060 rows for training data and 222352 rows for test data.

How did you decide what features to use in your analysis?
--------------------------------

Since I only have 4 features, there isn't any drastic reduction of features in my dataset.  The PhraseId was removed because it provides no value to the data in this application.

What challenges did you face in terms of obtaining and organizing the data?
--------------------------------

The biggest challenge faced with this data was being able to familiarize myself with the python tools to accomplish the data changes I wanted.  The removal of unneeded features such as the SentenceId, PhraseId and separate Sentiment were not too difficult.  Accomplishing the parsing of the words in each phrase was the most challenging part.  Understanding what the libraries outputted into what datatype was challenging as each datatype class has their own functions that needed a deeper understanding to better manage the data.

Further additions would be to take out stop words in the parsed out phrases as they are not needed and could overfit my results.


Describe what kinds of statistical methods you used, and perhaps others you considered but did not use, and how you decided what to use.
--------------------------------

My data set had me conclude that the best model was the Naive Bayes model to solve my sentiment analysis problem.   Throught he libraries provided in python, I had the ability to use different types of Naive Bayes formulas and compare the results.   The attempts I made were as follows.

Attempt A
--
naive_bayes.MultinomialNB on the whole phrase.  No parsing of the individual words into the model.

Attempt B
--
Gaussian Model didn't work well and caused the machine to crash my python program.  It was found out later that this model cannot handle sparse data very well so it was unfit for my type of data.  This was an attempt to better understand these programs and try new things so I was not discouraged by this finding.

Attempt C
--
naive_bayes.BernoulliNB on the whole phrase.  No parsing of the individual words into the model.

Attempt D
--
 nltk.classify NaiveBayesClassifier

Attempt E
--
DecisionTreeClassifier

Attempt classification comparison
--


What business applications do your findings have?
--------------------------------

